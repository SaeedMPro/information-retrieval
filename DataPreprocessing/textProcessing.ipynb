{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Media Sentiments Analysis Dataset Report\n",
    "\n",
    "---\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "This dataset contains a collection of social media posts labeled with sentiment classes. The primary goal of this dataset is to facilitate training and evaluation of Natural Language Processing (NLP) models for **sentiment analysis**. Each entry in the dataset represents a short piece of user-generated content, typically a tweet or a post, annotated with a sentiment label.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dataset Source\n",
    "\n",
    "- **URL**: [Kaggle Dataset](https://www.kaggle.com/datasets/kashishparmar02/social-media-sentiments-analysis-dataset)\n",
    "- **Author**: [Kashish Parmar](https://www.kaggle.com/kashishparmar02)\n",
    "\n",
    "---\n",
    "\n",
    "#### Columns Description\n",
    "\n",
    "| Column Name | Description                              |\n",
    "|-------------|------------------------------------------|\n",
    "| `text`      | The actual text from the social media post |\n",
    "| `sentiment` | The sentiment label (e.g., Positive, Neutral, Negative) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Download NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\SaeedM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\SaeedM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SaeedM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SaeedM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pip install nltk\n",
    "\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Display Top 10 Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Just finished an amazing workout! ğŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Fitness #Workout</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Travel #Adventure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Cooking #Food</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Feeling grateful for the little things in lif...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-16 09:10:00</td>\n",
       "      <td>GratitudeNow</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Gratitude #PositiveVibes</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>India</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Rainy days call for cozy blankets and hot coc...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-16 14:45:00</td>\n",
       "      <td>RainyDays</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#RainyDays #Cozy</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>The new movie release is a must-watch!       ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-16 19:30:00</td>\n",
       "      <td>MovieBuff</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#MovieNight #MustWatch</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Political discussions heating up on the timel...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-17 08:00:00</td>\n",
       "      <td>DebateTalk</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Politics #Debate</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Missing summer vibes and beach days.         ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-17 12:20:00</td>\n",
       "      <td>BeachLover</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Summer #BeachDays</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "5             5           5   \n",
       "6             6           6   \n",
       "7             7           7   \n",
       "8             8           8   \n",
       "9             9           9   \n",
       "\n",
       "                                                Text    Sentiment  \\\n",
       "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1   Traffic was terrible this morning.           ...   Negative     \n",
       "2   Just finished an amazing workout! ğŸ’ª          ...   Positive     \n",
       "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "5   Feeling grateful for the little things in lif...   Positive     \n",
       "6   Rainy days call for cozy blankets and hot coc...   Positive     \n",
       "7   The new movie release is a must-watch!       ...   Positive     \n",
       "8   Political discussions heating up on the timel...   Negative     \n",
       "9   Missing summer vibes and beach days.         ...   Neutral      \n",
       "\n",
       "             Timestamp            User     Platform  \\\n",
       "0  2023-01-15 12:30:00   User123          Twitter     \n",
       "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
       "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
       "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
       "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
       "5  2023-01-16 09:10:00   GratitudeNow     Twitter     \n",
       "6  2023-01-16 14:45:00   RainyDays        Facebook    \n",
       "7  2023-01-16 19:30:00   MovieBuff       Instagram    \n",
       "8  2023-01-17 08:00:00   DebateTalk       Twitter     \n",
       "9  2023-01-17 12:20:00   BeachLover       Facebook    \n",
       "\n",
       "                                     Hashtags  Retweets  Likes       Country  \\\n",
       "0   #Nature #Park                                  15.0   30.0     USA         \n",
       "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
       "2   #Fitness #Workout                              20.0   40.0   USA           \n",
       "3   #Travel #Adventure                              8.0   15.0     UK          \n",
       "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
       "5     #Gratitude #PositiveVibes                    25.0   50.0     India       \n",
       "6   #RainyDays #Cozy                               10.0   20.0     Canada      \n",
       "7    #MovieNight #MustWatch                        15.0   30.0       USA       \n",
       "8    #Politics #Debate                             30.0   60.0     USA         \n",
       "9    #Summer #BeachDays                            18.0   35.0    Australia    \n",
       "\n",
       "   Year  Month  Day  Hour  \n",
       "0  2023      1   15    12  \n",
       "1  2023      1   15     8  \n",
       "2  2023      1   15    15  \n",
       "3  2023      1   15    18  \n",
       "4  2023      1   15    19  \n",
       "5  2023      1   16     9  \n",
       "6  2023      1   16    14  \n",
       "7  2023      1   16    19  \n",
       "8  2023      1   17     8  \n",
       "9  2023      1   17    12  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"SocialMedia.csv\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout! ğŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    Sentiment\n",
       "0   Enjoying a beautiful day at the park!        ...   Positive  \n",
       "1   Traffic was terrible this morning.           ...   Negative  \n",
       "2   Just finished an amazing workout! ğŸ’ª          ...   Positive  \n",
       "3   Excited about the upcoming weekend getaway!  ...   Positive  \n",
       "4   Trying out a new recipe for dinner tonight.  ...   Neutral   "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df[['Text', 'Sentiment']] \n",
    "df.dropna(subset=['Text', 'Sentiment'], inplace=True) \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.wl = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.html_tags = re.compile(r'<.*?>')\n",
    "        self.punctuations = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "        self.extra_spaces = re.compile(r'\\s+')\n",
    "        self.digits = re.compile(r'\\d')\n",
    "        self.brackets_numbers = re.compile(r'\\[[0-9]*\\]')\n",
    "        self.special_chars = re.compile(r\"[*/&|_<>~+=\\\\^â„¢%\\\"â€â€œââ€]+\")\n",
    "        self.unwanted_chars = re.compile(r\"[à¤‚-à±‹Ì‡â€¢ã€‘ã€\\{\\}\\(\\)\\[\\]â€¼.,;:?!â€¦]+\")\n",
    "\n",
    "    def word_remove(self, text):\n",
    "        return re.sub(r'\\n\\s*|http\\S+', '', text)\n",
    "\n",
    "    def char_replacing(self, text):\n",
    "        text = re.sub(r\"[â€˜Â´â€™Ì‡]+\", \"'\", text)\n",
    "        text = re.sub(r\"[#Ì‡]+\", \"#\", text)\n",
    "        return re.sub(r\"[â€â€œââ€\\\"]\", \"\\\"\", text)\n",
    "\n",
    "    def word_expanding(self, text):\n",
    "        contractions = {\n",
    "            r\"(\\b)([Ii])'m\": r\"\\1\\2 am\",\n",
    "            r\"(\\b)([Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou)'re\": r\"\\1\\2 are\",\n",
    "            r\"(\\b)([Ll]et)'s\": r\"\\1\\2 us\",\n",
    "            r\"(\\b)([Hh]e|[Ii]|[Ss]he|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Yy]ou)'ll\": r\"\\1\\2 will\",\n",
    "            r\"(\\b)([Ii]|[Ss]hould|[Tt]hey|[Ww]e|[Ww]hat|[Ww]ho|[Ww]ould|[Yy]ou)'ve\": r\"\\1\\2 have\",\n",
    "            r\"'d\": \" would\",\n",
    "            r\"'s\": \" is\",\n",
    "            r\"isn't\": \"is not\",\n",
    "            r\" its \": \" it is \"\n",
    "        }\n",
    "        for pattern, repl in contractions.items():\n",
    "            text = re.sub(pattern, repl, text)\n",
    "        return text\n",
    "\n",
    "    def word_negation(self, text):\n",
    "        negations = {\n",
    "            r\"(\\b)([Aa]re|[Cc]ould|[Dd]id|[Dd]oes|[Dd]o|[Hh]ad|[Hh]as|[Hh]ave|[Ii]s|[Mm]ight|[Mm]ust|[Ss]hould|[Ww]ere|[Ww]as|[Ww]ould)n't\": r\"\\1\\2 not\",\n",
    "            r\"(\\b)([Cc]a)n't\": r\"\\1\\2n not\",\n",
    "            r\"(\\b)([Ww])on't\": r\"\\1\\2ill not\",\n",
    "            r\"(\\b)([Ss])han't\": r\"\\1\\2hall not\"\n",
    "        }\n",
    "        for pattern, repl in negations.items():\n",
    "            text = re.sub(pattern, repl, text)\n",
    "        return text\n",
    "\n",
    "    def char_removing(self, text):\n",
    "        text = self.unwanted_chars.sub(\"\", text)\n",
    "        text = self.special_chars.sub(\" \", text)\n",
    "        text = self.digits.sub(\" \", text)\n",
    "        text = self.brackets_numbers.sub(\" \", text)\n",
    "        return text\n",
    "\n",
    "    def word_stopwords(self, text):\n",
    "        return ' '.join(word for word in text.split() if word not in self.stop_words)\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        return {\n",
    "            'J': wordnet.ADJ, 'V': wordnet.VERB,\n",
    "            'N': wordnet.NOUN, 'R': wordnet.ADV\n",
    "        }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    def lemmatization(self, text):\n",
    "        word_pos_tags = nltk.pos_tag(word_tokenize(text))\n",
    "        return \" \".join(self.wl.lemmatize(word, self.get_wordnet_pos(pos)) for word, pos in word_pos_tags)\n",
    "\n",
    "    def stemming(self, text):\n",
    "        return \" \".join(self.stemmer.stem(word) for word in word_tokenize(text))\n",
    "    \n",
    "    def emoji_categorization(self, text):\n",
    "        text = re.sub(r\"[â˜ºâ˜»ğŸ˜ŠğŸ˜ŒğŸ™‚]+\", \"ğŸ™‚\", text)\n",
    "        text = re.sub(r\"[ğŸ˜€ğŸ˜ğŸ˜†ğŸ˜„ğŸ˜ƒğŸ˜¸ğŸ˜º]+\", \"ğŸ˜€\", text)\n",
    "        text = re.sub(r\"[â˜¹ğŸ˜ğŸ˜”ğŸ™]+\", \"ğŸ™\", text)\n",
    "        text = re.sub(r\"[â™¥â¤â™¡ğŸ’ŸğŸ’ğŸ’œğŸ’›ğŸ’šğŸ’™ğŸ–¤ğŸ’˜ğŸ’—ğŸ’–ğŸ’•ğŸ’“ğŸ’ğŸ’Œ]+\", \"ğŸ’œ\", text)\n",
    "        text = re.sub(r\"[ğŸ˜—ğŸ˜™ğŸ˜šğŸ˜ğŸ˜½ğŸ˜»ğŸ˜˜]+\", \"ğŸ˜˜\", text)\n",
    "        text = re.sub(r\"[ğŸ˜®ğŸ˜¯ğŸ˜²ğŸ™€]+\", \"ğŸ˜®\", text)\n",
    "        text = re.sub(r\"[ğŸ˜¨ğŸ˜§ğŸ˜¦]+\", \"ğŸ˜¦\", text)\n",
    "        text = re.sub(r\"[ğŸ˜]+\", \"ğŸ˜\", text)\n",
    "        text = re.sub(r\"[ğŸ˜œğŸ˜ğŸ˜›]+\", \"ğŸ˜›\", text)\n",
    "        text = re.sub(r\"[ğŸ¤£ğŸ˜¹ğŸ˜‚]+\", \"ğŸ˜‚\", text)\n",
    "        text = re.sub(r\"[ğŸ˜¿ğŸ˜¢ğŸ˜­ğŸ˜¥ğŸ˜ªğŸ˜¢]+\", \"ğŸ˜¢\", text)\n",
    "        text = re.sub(r\"[ğŸ˜ ğŸ˜¾ğŸ˜¤ğŸ‘¿ğŸ˜¡]+\", \"ğŸ˜¡\", text)\n",
    "        text = re.sub(r\"[ğŸ‘¬ğŸ‘­ğŸ‘«]+\", \"ğŸ‘«\", text)\n",
    "        text = re.sub(r\"[âœ”]+\", \"âœ…\", text)\n",
    "        text = re.sub(r\"[ğŸŒ]+\", \"â˜€\", text)\n",
    "        text = re.sub(r\"[ğŸŠğŸ‰ğŸˆğŸ‚ğŸ†ğŸ‡]+\", \"ğŸ‰\", text)\n",
    "        text = re.sub(r\"[âš½âš¾ğŸ€ğŸğŸˆğŸ‰ğŸ¾ğŸ³ğŸğŸ‘ğŸ’ğŸ“ğŸ¸ğŸ¥Šâ›³ğŸŠğŸŒğŸƒğŸ„ğŸ¿]+\", \" :sport: \", text)\n",
    "        text = re.sub(r\"[ğŸŒ‘ğŸŒ“ğŸŒ•ğŸŒ™ğŸŒœğŸŒ›ğŸŒ]+\", \" :moon: \", text)\n",
    "        text = re.sub(r\"[ğŸŒğŸŒğŸŒ]+\", \" :earth: \", text)\n",
    "        text = re.sub(r\"[ğŸ‚ğŸ„ğŸ…ğŸ‡ğŸˆğŸ‰ğŸŠğŸ‹ğŸğŸğŸğŸ‘ğŸ’ğŸ“ğŸ”ğŸ•ğŸ–ğŸ—ğŸ˜ğŸšğŸ›ğŸğŸğŸŸğŸ ğŸ¢ğŸ£ğŸ¥ğŸ¦ğŸ¨ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ğŸ´ğŸµğŸ¶ğŸ·ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼]+\", \" :animal: \", text)\n",
    "        text = re.sub(r\"[ğŸ„ğŸ…ğŸ†ğŸ‡ğŸ‰ğŸŠğŸŒğŸğŸğŸğŸ‘ğŸ’ğŸ“]+\", \" :fruit: \", text)\n",
    "        text = re.sub(r\"[ğŸ”ğŸ•ğŸ–ğŸ—ğŸ›ğŸœğŸğŸğŸŸğŸ£ğŸ¥ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ¯ğŸ°]+\", \" :food: \", text)\n",
    "        text = re.sub(r\"[ğŸ‡¦-ğŸ‡¿]{2}\", \" :flag: \", text)\n",
    "        text = re.sub(r\"[â™©â™ªâ™«â™¬ğŸµğŸ¶ğŸ·ğŸ¸ğŸ¹ğŸºğŸ¼ğŸ¤ğŸ§ğŸ»]+\", \" :music: \", text)\n",
    "        text = re.sub(r\"[ğŸŒ·ğŸŒ¸ğŸŒ¹ğŸŒºğŸŒ»ğŸŒ¼]+\", \" :flower: \", text)\n",
    "        text = re.sub(r\"[ğŸŒ±ğŸŒ²ğŸŒ³ğŸŒ´ğŸŒµğŸŒ¾ğŸŒ¿ğŸ€ğŸğŸ‚ğŸƒ]+\", \" :plant: \", text)\n",
    "        text = re.sub(r\"[ğŸ·ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ¾]+\", \" :drink: \", text)\n",
    "        text = re.sub(r\"[ğŸ‘•ğŸ‘—ğŸ‘™ğŸ‘šğŸ‘›ğŸ‘œğŸ‘ ]+\", \" :dress: \", text)\n",
    "        text = re.sub(r\"[ğŸ’°ğŸ’³ğŸ’µğŸ’·ğŸ’¸]+\", \" :money: \", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def emoticon_to_emoji(self,text):\n",
    "        text = re.sub(r\":-*\\)+\", \"ğŸ™‚\", text)\n",
    "        text = re.sub(r\"\\(+-*:\", \"ğŸ™‚\", text)\n",
    "        text = re.sub(r\":-*(d|D)+\", \"ğŸ˜€\", text)\n",
    "        text = re.sub(r\"x-*(d|D)+\", \"ğŸ˜€\", text)\n",
    "        text = re.sub(r\":-*(p|P)+\", \"ğŸ˜›\", text)\n",
    "        text = re.sub(r\":-*\\(+\", \"ğŸ™\", text)\n",
    "        text = re.sub(r\";-*\\)+\", \"ğŸ˜‰\", text)\n",
    "        text = re.sub(r\":-*<+\", \"ğŸ˜ \", text)\n",
    "        text = re.sub(r\":-*/+\", \"ğŸ˜•\", text)\n",
    "        text = re.sub(r\":-*\\*+\", \"ğŸ˜˜\", text)\n",
    "        text = re.sub(r\":-*(o|O)+\", \"ğŸ˜®\", text)\n",
    "        text = re.sub(r\":'+-*\\)+\", \"ğŸ˜‚\", text)\n",
    "        text = re.sub(r\":'+-*\\(+\", \"ğŸ˜¢\", text)\n",
    "        text = re.sub(r\">_<\", \"ğŸ˜£\", text)\n",
    "        text = re.sub(r\"\\(-_-\\)zzz\", \"ğŸ˜´\", text)\n",
    "        text = re.sub(r\"-_+-\", \"ğŸ˜‘\", text)\n",
    "        text = re.sub(r\"\\^_+\\^\", \"ğŸ˜Š\", text)\n",
    "        text = re.sub(r\"\\*_+\\*\", \"ğŸ˜\", text)\n",
    "        text = re.sub(r\">_+>\", \"ğŸ˜’\", text)\n",
    "        text = re.sub(r\"<_+<\", \"ğŸ˜’\", text)\n",
    "        text = re.sub(r\"\\(âŒ£Ì_âŒ£Ì€\\)\", \"ğŸ˜Œ\", text)\n",
    "        text = re.sub(r\";_+;\", \"ğŸ˜¢\", text)\n",
    "        text = re.sub(r\"3:-+\\)\", \"ğŸ˜ˆ\", text)\n",
    "        text = re.sub(r\"<+3+\", \"ğŸ’œ\", text)\n",
    "        text = re.sub(r\">\\.<\", \"ğŸ¤”\", text)\n",
    "        text = re.sub(r\"\\._+\\.\", \"ğŸ˜”\", text)\n",
    "        text = re.sub(r\"Â¯\\\\_\\(ãƒ„\\)_/Â¯\", \"ğŸ¤·\", text)\n",
    "        text = re.sub(r\"Â¯_\\(ãƒ„\\)_/Â¯\", \"ğŸ’\", text)\n",
    "        text = re.sub(r\"(o|O)+_+(o|O)+\", \"ğŸ˜\", text)\n",
    "        text = re.sub(r\"(o|O)+\\.+(o|O)+\", \"ğŸ˜®\", text)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower().strip()\n",
    "        text = self.html_tags.sub('', text)  \n",
    "        text = self.punctuations.sub(' ', text) \n",
    "        text = self.extra_spaces.sub(' ', text)  \n",
    "        text = self.word_remove(text)\n",
    "        text = self.char_replacing(text)\n",
    "        text = self.word_expanding(text)\n",
    "        text = self.word_negation(text)\n",
    "        text = self.emoji_categorization(text)  \n",
    "        text = self.emoticon_to_emoji(text) \n",
    "        text = self.char_removing(text)\n",
    "        text = self.extra_spaces.sub(\" \", text)\n",
    "        text = self.word_stopwords(text)\n",
    "        return text\n",
    "\n",
    "    def preprocessing(self, text):\n",
    "        return self.preprocess_text(text)\n",
    "\n",
    "# def prepare_dataset(doc):\n",
    "#   txt=Preprocessor().preprocessing(doc)\n",
    "#   print(txt)\n",
    "#   return txt\n",
    "\n",
    "# df['text_clean'] = df['Text'].iloc[:100].apply(prepare_dataset)\n",
    "\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Stemmed</th>\n",
       "      <th>Text_Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>enjoying beautiful day park</td>\n",
       "      <td>enjoy beauti day park</td>\n",
       "      <td>enjoy beautiful day park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>traffic terrible morning</td>\n",
       "      <td>traffic terribl morn</td>\n",
       "      <td>traffic terrible morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout! ğŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>finished amazing workout ğŸ’ª</td>\n",
       "      <td>finish amaz workout ğŸ’ª</td>\n",
       "      <td>finish amaze workout ğŸ’ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>excited upcoming weekend getaway</td>\n",
       "      <td>excit upcom weekend getaway</td>\n",
       "      <td>excited upcoming weekend getaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>trying new recipe dinner tonight</td>\n",
       "      <td>tri new recip dinner tonight</td>\n",
       "      <td>try new recipe dinner tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feeling grateful for the little things in lif...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>feeling grateful little things life</td>\n",
       "      <td>feel grate littl thing life</td>\n",
       "      <td>feel grateful little thing life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rainy days call for cozy blankets and hot coc...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>rainy days call cozy blankets hot cocoa</td>\n",
       "      <td>raini day call cozi blanket hot cocoa</td>\n",
       "      <td>rainy day call cozy blanket hot cocoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The new movie release is a must-watch!       ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>new movie release must watch</td>\n",
       "      <td>new movi releas must watch</td>\n",
       "      <td>new movie release must watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Political discussions heating up on the timel...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>political discussions heating timeline</td>\n",
       "      <td>polit discuss heat timelin</td>\n",
       "      <td>political discussion heat timeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Missing summer vibes and beach days.         ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>missing summer vibes beach days</td>\n",
       "      <td>miss summer vibe beach day</td>\n",
       "      <td>miss summer vibe beach day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    Sentiment  \\\n",
       "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1   Traffic was terrible this morning.           ...   Negative     \n",
       "2   Just finished an amazing workout! ğŸ’ª          ...   Positive     \n",
       "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "5   Feeling grateful for the little things in lif...   Positive     \n",
       "6   Rainy days call for cozy blankets and hot coc...   Positive     \n",
       "7   The new movie release is a must-watch!       ...   Positive     \n",
       "8   Political discussions heating up on the timel...   Negative     \n",
       "9   Missing summer vibes and beach days.         ...   Neutral      \n",
       "\n",
       "                                Text_Clean  \\\n",
       "0              enjoying beautiful day park   \n",
       "1                 traffic terrible morning   \n",
       "2               finished amazing workout ğŸ’ª   \n",
       "3         excited upcoming weekend getaway   \n",
       "4         trying new recipe dinner tonight   \n",
       "5      feeling grateful little things life   \n",
       "6  rainy days call cozy blankets hot cocoa   \n",
       "7             new movie release must watch   \n",
       "8   political discussions heating timeline   \n",
       "9          missing summer vibes beach days   \n",
       "\n",
       "                            Text_Stemmed  \\\n",
       "0                  enjoy beauti day park   \n",
       "1                   traffic terribl morn   \n",
       "2                  finish amaz workout ğŸ’ª   \n",
       "3            excit upcom weekend getaway   \n",
       "4           tri new recip dinner tonight   \n",
       "5            feel grate littl thing life   \n",
       "6  raini day call cozi blanket hot cocoa   \n",
       "7             new movi releas must watch   \n",
       "8             polit discuss heat timelin   \n",
       "9             miss summer vibe beach day   \n",
       "\n",
       "                         Text_Lemmatized  \n",
       "0               enjoy beautiful day park  \n",
       "1               traffic terrible morning  \n",
       "2                 finish amaze workout ğŸ’ª  \n",
       "3       excited upcoming weekend getaway  \n",
       "4          try new recipe dinner tonight  \n",
       "5        feel grateful little thing life  \n",
       "6  rainy day call cozy blanket hot cocoa  \n",
       "7           new movie release must watch  \n",
       "8     political discussion heat timeline  \n",
       "9             miss summer vibe beach day  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "\n",
    "df['Text_Clean'] = df['Text'].apply(preprocessor.preprocess_text)\n",
    "df['Text_Stemmed'] = df['Text_Clean'].apply(preprocessor.stemming)\n",
    "df['Text_Lemmatized'] = df['Text_Clean'].apply(preprocessor.lemmatization)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"SocialMedia_processed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
